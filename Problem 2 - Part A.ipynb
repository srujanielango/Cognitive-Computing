{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 (Team 4) \n",
    "#### Experiment with different parameters with a conv-2d layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.8598 - acc: 0.3177 - val_loss: 1.5607 - val_acc: 0.4365\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5730 - acc: 0.4228 - val_loss: 1.3960 - val_acc: 0.4974\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4358 - acc: 0.4824 - val_loss: 1.2483 - val_acc: 0.5622\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.3359 - acc: 0.5212 - val_loss: 1.2292 - val_acc: 0.5639\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.2636 - acc: 0.5490 - val_loss: 1.1042 - val_acc: 0.6134\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.2017 - acc: 0.5731 - val_loss: 1.0293 - val_acc: 0.6410\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.1483 - acc: 0.5929 - val_loss: 0.9784 - val_acc: 0.6609\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.1059 - acc: 0.6111 - val_loss: 0.9738 - val_acc: 0.6647\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.0622 - acc: 0.6239 - val_loss: 0.9133 - val_acc: 0.6866\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.0300 - acc: 0.6381 - val_loss: 0.8873 - val_acc: 0.6920\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.0037 - acc: 0.6468 - val_loss: 0.8758 - val_acc: 0.7014\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9781 - acc: 0.6549 - val_loss: 0.8522 - val_acc: 0.7019\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.9555 - acc: 0.6652 - val_loss: 0.8305 - val_acc: 0.7153\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.9343 - acc: 0.6725 - val_loss: 0.8067 - val_acc: 0.7232\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9151 - acc: 0.6800 - val_loss: 0.7803 - val_acc: 0.7331\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9010 - acc: 0.6857 - val_loss: 0.7684 - val_acc: 0.7363\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8881 - acc: 0.6907 - val_loss: 0.7935 - val_acc: 0.7239\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8735 - acc: 0.6964 - val_loss: 0.7610 - val_acc: 0.7360\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8659 - acc: 0.6985 - val_loss: 0.7172 - val_acc: 0.7563\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8493 - acc: 0.7046 - val_loss: 0.7657 - val_acc: 0.7360\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.8497 - acc: 0.7041 - val_loss: 0.7302 - val_acc: 0.7481\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8332 - acc: 0.7083 - val_loss: 0.7240 - val_acc: 0.7538\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8296 - acc: 0.7104 - val_loss: 0.7009 - val_acc: 0.7591\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8190 - acc: 0.7163 - val_loss: 0.7114 - val_acc: 0.7564\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8088 - acc: 0.7207 - val_loss: 0.7022 - val_acc: 0.7640\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8091 - acc: 0.7203 - val_loss: 0.6854 - val_acc: 0.7680\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8002 - acc: 0.7218 - val_loss: 0.6758 - val_acc: 0.7666\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8028 - acc: 0.7224 - val_loss: 0.7010 - val_acc: 0.7562\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.7933 - acc: 0.7248 - val_loss: 0.6894 - val_acc: 0.7636\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.7880 - acc: 0.7297 - val_loss: 0.7049 - val_acc: 0.7600\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7825 - acc: 0.7324 - val_loss: 0.6746 - val_acc: 0.7709\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7804 - acc: 0.7285 - val_loss: 0.6599 - val_acc: 0.7767\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7722 - acc: 0.7351 - val_loss: 0.6730 - val_acc: 0.7709\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7711 - acc: 0.7347 - val_loss: 0.6828 - val_acc: 0.7709\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7740 - acc: 0.7355 - val_loss: 0.6746 - val_acc: 0.7715\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7695 - acc: 0.7377 - val_loss: 0.6827 - val_acc: 0.7775\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.7659 - acc: 0.7388 - val_loss: 0.6594 - val_acc: 0.7839\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.7676 - acc: 0.7377 - val_loss: 0.6434 - val_acc: 0.7856\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7638 - acc: 0.7394 - val_loss: 0.6819 - val_acc: 0.7811\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7604 - acc: 0.7388 - val_loss: 0.6565 - val_acc: 0.7854\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7569 - acc: 0.7417 - val_loss: 0.6370 - val_acc: 0.7844\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7581 - acc: 0.7422 - val_loss: 0.6685 - val_acc: 0.7833\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7548 - acc: 0.7425 - val_loss: 0.6492 - val_acc: 0.7899\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7551 - acc: 0.7422 - val_loss: 0.6972 - val_acc: 0.7696\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.7520 - acc: 0.7460 - val_loss: 0.6515 - val_acc: 0.7858\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7543 - acc: 0.7426 - val_loss: 0.6401 - val_acc: 0.7902\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.7540 - acc: 0.7454 - val_loss: 0.6446 - val_acc: 0.7861\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7492 - acc: 0.7459 - val_loss: 0.6422 - val_acc: 0.7820\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7530 - acc: 0.7438 - val_loss: 0.6311 - val_acc: 0.7953\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7524 - acc: 0.7463 - val_loss: 0.6504 - val_acc: 0.7829\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7487 - acc: 0.7471 - val_loss: 0.6174 - val_acc: 0.7965\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7444 - acc: 0.7485 - val_loss: 0.6406 - val_acc: 0.7832\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7502 - acc: 0.7477 - val_loss: 0.6512 - val_acc: 0.7954\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7529 - acc: 0.7449 - val_loss: 0.6358 - val_acc: 0.7875\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7516 - acc: 0.7468 - val_loss: 0.6587 - val_acc: 0.7880\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7512 - acc: 0.7498 - val_loss: 0.6369 - val_acc: 0.7912\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7440 - acc: 0.7492 - val_loss: 0.7200 - val_acc: 0.7789\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7493 - acc: 0.7464 - val_loss: 0.6497 - val_acc: 0.7805\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7501 - acc: 0.7478 - val_loss: 0.6427 - val_acc: 0.7909\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7463 - acc: 0.7500 - val_loss: 0.6173 - val_acc: 0.7993\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.7493 - acc: 0.7488 - val_loss: 0.6587 - val_acc: 0.7813\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.7497 - acc: 0.7475 - val_loss: 0.6346 - val_acc: 0.7939\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7468 - acc: 0.7504 - val_loss: 0.6819 - val_acc: 0.7854\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7477 - acc: 0.7490 - val_loss: 0.6386 - val_acc: 0.7933\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7513 - acc: 0.7503 - val_loss: 0.6682 - val_acc: 0.7779\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7506 - acc: 0.7500 - val_loss: 0.6328 - val_acc: 0.7914\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7493 - acc: 0.7478 - val_loss: 0.6329 - val_acc: 0.7901\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7589 - acc: 0.7487 - val_loss: 0.6517 - val_acc: 0.7852\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7592 - acc: 0.7485 - val_loss: 0.6769 - val_acc: 0.7856\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7556 - acc: 0.7469 - val_loss: 0.6650 - val_acc: 0.7853\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.7526 - acc: 0.7479 - val_loss: 0.6537 - val_acc: 0.7866\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7622 - acc: 0.7460 - val_loss: 0.6821 - val_acc: 0.7811\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7540 - acc: 0.7484 - val_loss: 0.6568 - val_acc: 0.7888\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7628 - acc: 0.7470 - val_loss: 0.6792 - val_acc: 0.7809\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7575 - acc: 0.7500 - val_loss: 0.6752 - val_acc: 0.7896\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7517 - acc: 0.7504 - val_loss: 0.6280 - val_acc: 0.7931\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.7632 - acc: 0.7461 - val_loss: 0.6436 - val_acc: 0.7974\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7638 - acc: 0.7479 - val_loss: 0.6331 - val_acc: 0.7963\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7639 - acc: 0.7455 - val_loss: 0.6304 - val_acc: 0.7952\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7652 - acc: 0.7463 - val_loss: 0.7141 - val_acc: 0.7939\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7691 - acc: 0.7431 - val_loss: 0.6422 - val_acc: 0.8024\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7659 - acc: 0.7452 - val_loss: 0.6124 - val_acc: 0.7973\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7691 - acc: 0.7430 - val_loss: 0.6542 - val_acc: 0.7844\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7757 - acc: 0.7442 - val_loss: 0.6483 - val_acc: 0.7909\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.7721 - acc: 0.7458 - val_loss: 0.6601 - val_acc: 0.7865\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7714 - acc: 0.7454 - val_loss: 0.7496 - val_acc: 0.7624\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7765 - acc: 0.7435 - val_loss: 0.6477 - val_acc: 0.7861\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7751 - acc: 0.7441 - val_loss: 0.7333 - val_acc: 0.7630\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7746 - acc: 0.7453 - val_loss: 0.6689 - val_acc: 0.7908\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7791 - acc: 0.7437 - val_loss: 0.6537 - val_acc: 0.7912\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7804 - acc: 0.7417 - val_loss: 0.6175 - val_acc: 0.7957\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7860 - acc: 0.7410 - val_loss: 0.6962 - val_acc: 0.7877\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7928 - acc: 0.7384 - val_loss: 0.7318 - val_acc: 0.7689\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7867 - acc: 0.7418 - val_loss: 0.6661 - val_acc: 0.7893\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7887 - acc: 0.7425 - val_loss: 0.7478 - val_acc: 0.7760\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7887 - acc: 0.7423 - val_loss: 0.6388 - val_acc: 0.7909\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8003 - acc: 0.7387 - val_loss: 0.7459 - val_acc: 0.7532\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.7976 - acc: 0.7401 - val_loss: 0.7263 - val_acc: 0.7735\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8048 - acc: 0.7390 - val_loss: 0.6824 - val_acc: 0.7764\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8015 - acc: 0.7376 - val_loss: 0.7658 - val_acc: 0.7601\n",
      "Saved trained model at D:\\Spring 2018\\AI\\Assignment-1\\saved_models\\keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 1s 130us/step\n",
      "Test loss: 0.765802329826355\n",
      "Test accuracy: 0.7601\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Changing the number of filters\n",
    "\n",
    "By default we start with 32 filters but now we are gona start with 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwa\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 1.7737 - acc: 0.3464 - val_loss: 1.4263 - val_acc: 0.4751\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 1.4540 - acc: 0.4743 - val_loss: 1.2237 - val_acc: 0.5686\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 49s 32ms/step - loss: 1.2932 - acc: 0.5377 - val_loss: 1.0758 - val_acc: 0.6209\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 1.1755 - acc: 0.5835 - val_loss: 1.0172 - val_acc: 0.6437\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 1.0853 - acc: 0.6173 - val_loss: 0.9080 - val_acc: 0.6840\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 1.0227 - acc: 0.6406 - val_loss: 0.8435 - val_acc: 0.7074\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 0.9682 - acc: 0.6603 - val_loss: 0.8247 - val_acc: 0.7145\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.9192 - acc: 0.6803 - val_loss: 0.7605 - val_acc: 0.7355\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 0.8811 - acc: 0.6925 - val_loss: 0.7512 - val_acc: 0.7381\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 43s 28ms/step - loss: 0.8539 - acc: 0.7027 - val_loss: 0.7218 - val_acc: 0.7521\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.8234 - acc: 0.7129 - val_loss: 0.7100 - val_acc: 0.7553\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 0.7991 - acc: 0.7233 - val_loss: 0.6959 - val_acc: 0.7647\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 0.7824 - acc: 0.7306 - val_loss: 0.7442 - val_acc: 0.7500\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.7717 - acc: 0.7328 - val_loss: 0.6363 - val_acc: 0.7829\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 0.7540 - acc: 0.7411 - val_loss: 0.6406 - val_acc: 0.7795\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.7434 - acc: 0.7457 - val_loss: 0.6456 - val_acc: 0.7801\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 43s 28ms/step - loss: 0.7292 - acc: 0.7509 - val_loss: 0.6315 - val_acc: 0.7862\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.7210 - acc: 0.7535 - val_loss: 0.6586 - val_acc: 0.7798\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 39s 25ms/step - loss: 0.7109 - acc: 0.7582 - val_loss: 0.6007 - val_acc: 0.7997\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.7095 - acc: 0.7596 - val_loss: 0.6013 - val_acc: 0.8015\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.7000 - acc: 0.7632 - val_loss: 0.6472 - val_acc: 0.7884\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 0.6993 - acc: 0.7645 - val_loss: 0.5780 - val_acc: 0.8074\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.6890 - acc: 0.7669 - val_loss: 0.6059 - val_acc: 0.7991\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 0.6856 - acc: 0.7688 - val_loss: 0.5805 - val_acc: 0.8072\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 0.6741 - acc: 0.7720 - val_loss: 0.5933 - val_acc: 0.7970\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.6661 - acc: 0.7745 - val_loss: 0.5767 - val_acc: 0.8050\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 0.6713 - acc: 0.7755 - val_loss: 0.5426 - val_acc: 0.8167\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 0.6646 - acc: 0.7779 - val_loss: 0.5604 - val_acc: 0.8121\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 41s 27ms/step - loss: 0.6563 - acc: 0.7784 - val_loss: 0.5598 - val_acc: 0.8087\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.6483 - acc: 0.7817 - val_loss: 0.5798 - val_acc: 0.8107\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.6504 - acc: 0.7830 - val_loss: 0.5412 - val_acc: 0.8192\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 0.6494 - acc: 0.7844 - val_loss: 0.5457 - val_acc: 0.8193\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.6471 - acc: 0.7843 - val_loss: 0.5722 - val_acc: 0.8141\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 0.6474 - acc: 0.7854 - val_loss: 0.5637 - val_acc: 0.8193\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.6434 - acc: 0.7872 - val_loss: 0.5384 - val_acc: 0.8237\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.6462 - acc: 0.7872 - val_loss: 0.5470 - val_acc: 0.8194\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 43s 27ms/step - loss: 0.6428 - acc: 0.7844 - val_loss: 0.5590 - val_acc: 0.8174\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.6331 - acc: 0.7894 - val_loss: 0.5527 - val_acc: 0.8210\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.6379 - acc: 0.7886 - val_loss: 0.5586 - val_acc: 0.8164\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.6387 - acc: 0.7885 - val_loss: 0.5957 - val_acc: 0.8082\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.6352 - acc: 0.7893 - val_loss: 0.6047 - val_acc: 0.8104\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.6359 - acc: 0.7908 - val_loss: 0.5257 - val_acc: 0.8273\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.6358 - acc: 0.7905 - val_loss: 0.5585 - val_acc: 0.8169\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.6352 - acc: 0.7891 - val_loss: 0.5191 - val_acc: 0.8277\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.6426 - acc: 0.7883 - val_loss: 0.5882 - val_acc: 0.8131\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.6436 - acc: 0.7908 - val_loss: 0.5677 - val_acc: 0.8131\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.6448 - acc: 0.7874 - val_loss: 0.5717 - val_acc: 0.8235\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.6474 - acc: 0.7875 - val_loss: 0.5554 - val_acc: 0.8222\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.6536 - acc: 0.7877 - val_loss: 0.5587 - val_acc: 0.8198\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 0.6471 - acc: 0.7891 - val_loss: 0.5915 - val_acc: 0.8216\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6501 - acc: 0.7871 - val_loss: 0.5337 - val_acc: 0.8259\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6551 - acc: 0.7875 - val_loss: 0.6432 - val_acc: 0.8202\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6643 - acc: 0.7856 - val_loss: 0.5442 - val_acc: 0.8239\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6635 - acc: 0.7833 - val_loss: 0.5840 - val_acc: 0.8100\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6635 - acc: 0.7848 - val_loss: 0.6328 - val_acc: 0.8118\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6699 - acc: 0.7827 - val_loss: 0.5932 - val_acc: 0.8241\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6707 - acc: 0.7848 - val_loss: 0.5849 - val_acc: 0.8222\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6704 - acc: 0.7842 - val_loss: 0.5858 - val_acc: 0.8065\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6818 - acc: 0.7800 - val_loss: 0.7116 - val_acc: 0.7768\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6888 - acc: 0.7789 - val_loss: 0.5778 - val_acc: 0.8230\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6945 - acc: 0.7769 - val_loss: 0.7077 - val_acc: 0.7763\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6883 - acc: 0.7785 - val_loss: 0.5933 - val_acc: 0.8119\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6962 - acc: 0.7745 - val_loss: 0.5738 - val_acc: 0.8244\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7011 - acc: 0.7757 - val_loss: 0.6212 - val_acc: 0.8115\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7064 - acc: 0.7723 - val_loss: 0.5865 - val_acc: 0.8088\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7123 - acc: 0.7720 - val_loss: 0.6621 - val_acc: 0.7910\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7194 - acc: 0.7694 - val_loss: 0.7490 - val_acc: 0.7651\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7283 - acc: 0.7677 - val_loss: 0.5760 - val_acc: 0.8177\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7286 - acc: 0.7653 - val_loss: 0.6557 - val_acc: 0.7971\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7428 - acc: 0.7638 - val_loss: 0.6796 - val_acc: 0.7714\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7514 - acc: 0.7641 - val_loss: 0.6651 - val_acc: 0.7954\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7526 - acc: 0.7610 - val_loss: 0.8618 - val_acc: 0.7500\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7584 - acc: 0.7597 - val_loss: 0.6751 - val_acc: 0.7980\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7716 - acc: 0.7571 - val_loss: 0.6970 - val_acc: 0.7782\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7740 - acc: 0.7549 - val_loss: 0.7282 - val_acc: 0.7741\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7885 - acc: 0.7505 - val_loss: 0.6908 - val_acc: 0.7840\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.7884 - acc: 0.7511 - val_loss: 0.6658 - val_acc: 0.7805\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.8013 - acc: 0.7482 - val_loss: 0.7396 - val_acc: 0.7560\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.8106 - acc: 0.7452 - val_loss: 0.7692 - val_acc: 0.7845\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.8220 - acc: 0.7428 - val_loss: 0.8475 - val_acc: 0.7611\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.8349 - acc: 0.7393 - val_loss: 0.7616 - val_acc: 0.7656\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.8426 - acc: 0.7353 - val_loss: 0.7303 - val_acc: 0.7843\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.8564 - acc: 0.7344 - val_loss: 0.6616 - val_acc: 0.7935\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.8553 - acc: 0.7324 - val_loss: 0.9229 - val_acc: 0.7168\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.8679 - acc: 0.7296 - val_loss: 0.7596 - val_acc: 0.7545\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.8747 - acc: 0.7287 - val_loss: 0.7275 - val_acc: 0.7778\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.8849 - acc: 0.7221 - val_loss: 0.7440 - val_acc: 0.7691\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.8997 - acc: 0.7196 - val_loss: 0.8464 - val_acc: 0.7586\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.9177 - acc: 0.7154 - val_loss: 0.7209 - val_acc: 0.7752\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.9258 - acc: 0.7139 - val_loss: 0.8366 - val_acc: 0.7458\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.9300 - acc: 0.7126 - val_loss: 1.4699 - val_acc: 0.6545\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.9427 - acc: 0.7077 - val_loss: 0.7977 - val_acc: 0.7379\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.9617 - acc: 0.7019 - val_loss: 0.8037 - val_acc: 0.7701\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.9635 - acc: 0.7006 - val_loss: 0.7496 - val_acc: 0.7825\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.9658 - acc: 0.7006 - val_loss: 0.7813 - val_acc: 0.7639\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.9932 - acc: 0.6930 - val_loss: 1.0506 - val_acc: 0.6403\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.9871 - acc: 0.6944 - val_loss: 0.8861 - val_acc: 0.7470\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.9995 - acc: 0.6906 - val_loss: 0.9528 - val_acc: 0.7483\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.0143 - acc: 0.6883 - val_loss: 0.9176 - val_acc: 0.7438\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.0243 - acc: 0.6858 - val_loss: 0.7900 - val_acc: 0.7602\n",
      "Saved trained model at D:\\Spring 2018\\AI\\Assignment-1\\saved_models\\keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 2s 190us/step\n",
      "Test loss: 0.7899943968772888\n",
      "Test accuracy: 0.7602\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is 76.02%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modifying the size of the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy decreases to 75% but the model fits well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.8637 - acc: 0.3189 - val_loss: 1.5900 - val_acc: 0.4249\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5804 - acc: 0.4255 - val_loss: 1.4821 - val_acc: 0.4727\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4562 - acc: 0.4736 - val_loss: 1.2899 - val_acc: 0.5405\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.3749 - acc: 0.5064 - val_loss: 1.2102 - val_acc: 0.5737\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.3103 - acc: 0.5312 - val_loss: 1.1940 - val_acc: 0.5710\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.2544 - acc: 0.5518 - val_loss: 1.1218 - val_acc: 0.6038\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.2124 - acc: 0.5675 - val_loss: 1.0692 - val_acc: 0.6209\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.1744 - acc: 0.5815 - val_loss: 1.0353 - val_acc: 0.6393\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.1381 - acc: 0.5965 - val_loss: 0.9821 - val_acc: 0.6559\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.1063 - acc: 0.6080 - val_loss: 0.9586 - val_acc: 0.6671\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.0841 - acc: 0.6150 - val_loss: 0.9270 - val_acc: 0.6773\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.0618 - acc: 0.6256 - val_loss: 0.9212 - val_acc: 0.6813\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.0419 - acc: 0.6307 - val_loss: 0.8867 - val_acc: 0.6869\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.0273 - acc: 0.6404 - val_loss: 0.8812 - val_acc: 0.6888\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.0139 - acc: 0.6425 - val_loss: 0.8809 - val_acc: 0.6907\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9984 - acc: 0.6489 - val_loss: 0.8870 - val_acc: 0.6870\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9912 - acc: 0.6532 - val_loss: 0.8889 - val_acc: 0.6902\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9726 - acc: 0.6577 - val_loss: 0.8412 - val_acc: 0.7139\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9753 - acc: 0.6608 - val_loss: 0.8263 - val_acc: 0.7153\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9676 - acc: 0.6640 - val_loss: 0.8428 - val_acc: 0.7039\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9599 - acc: 0.6669 - val_loss: 0.8625 - val_acc: 0.6973\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9491 - acc: 0.6710 - val_loss: 0.8146 - val_acc: 0.7176\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9497 - acc: 0.6700 - val_loss: 0.8224 - val_acc: 0.7191\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9453 - acc: 0.6726 - val_loss: 0.8125 - val_acc: 0.7243\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9398 - acc: 0.6732 - val_loss: 0.8565 - val_acc: 0.7193\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9376 - acc: 0.6750 - val_loss: 0.7973 - val_acc: 0.7308\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9286 - acc: 0.6782 - val_loss: 0.8079 - val_acc: 0.7348\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9308 - acc: 0.6788 - val_loss: 0.8371 - val_acc: 0.7142\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9266 - acc: 0.6814 - val_loss: 0.8087 - val_acc: 0.7243\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9182 - acc: 0.6835 - val_loss: 0.8140 - val_acc: 0.7291\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9236 - acc: 0.6817 - val_loss: 0.8026 - val_acc: 0.7396\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9171 - acc: 0.6833 - val_loss: 0.7863 - val_acc: 0.7486\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9148 - acc: 0.6861 - val_loss: 0.8064 - val_acc: 0.7380\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9094 - acc: 0.6882 - val_loss: 0.7600 - val_acc: 0.7443\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9103 - acc: 0.6873 - val_loss: 0.7714 - val_acc: 0.7453\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9019 - acc: 0.6897 - val_loss: 0.8272 - val_acc: 0.7453\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.9083 - acc: 0.6887 - val_loss: 0.7975 - val_acc: 0.7359\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8965 - acc: 0.6923 - val_loss: 0.7855 - val_acc: 0.7313\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8987 - acc: 0.6923 - val_loss: 0.7630 - val_acc: 0.7477\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8974 - acc: 0.6921 - val_loss: 0.7933 - val_acc: 0.7334\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8955 - acc: 0.6933 - val_loss: 0.7804 - val_acc: 0.7516\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8938 - acc: 0.6928 - val_loss: 0.7948 - val_acc: 0.7377\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8971 - acc: 0.6934 - val_loss: 0.7322 - val_acc: 0.7536\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8884 - acc: 0.6961 - val_loss: 0.7869 - val_acc: 0.7467\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8880 - acc: 0.6971 - val_loss: 0.8780 - val_acc: 0.7409\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8879 - acc: 0.6967 - val_loss: 0.7799 - val_acc: 0.7377\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8830 - acc: 0.6959 - val_loss: 0.8521 - val_acc: 0.7405\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8873 - acc: 0.6976 - val_loss: 0.8085 - val_acc: 0.7397\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8825 - acc: 0.6996 - val_loss: 0.8321 - val_acc: 0.7518\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8865 - acc: 0.6973 - val_loss: 0.8453 - val_acc: 0.7467\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8819 - acc: 0.6974 - val_loss: 0.7902 - val_acc: 0.7488\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8804 - acc: 0.6998 - val_loss: 0.7861 - val_acc: 0.7475\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8790 - acc: 0.7008 - val_loss: 0.8176 - val_acc: 0.7524\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8832 - acc: 0.6997 - val_loss: 0.7724 - val_acc: 0.7412\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8761 - acc: 0.7009 - val_loss: 0.8377 - val_acc: 0.7452\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8754 - acc: 0.7033 - val_loss: 0.8548 - val_acc: 0.7430\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8786 - acc: 0.7031 - val_loss: 0.7946 - val_acc: 0.7471\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8678 - acc: 0.7038 - val_loss: 0.7455 - val_acc: 0.7556\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8751 - acc: 0.7037 - val_loss: 0.8532 - val_acc: 0.7486\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8708 - acc: 0.7027 - val_loss: 0.8400 - val_acc: 0.7513\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8756 - acc: 0.7014 - val_loss: 0.7759 - val_acc: 0.7568\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8771 - acc: 0.7009 - val_loss: 0.7429 - val_acc: 0.7578\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8701 - acc: 0.7019 - val_loss: 0.8326 - val_acc: 0.7325\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8687 - acc: 0.7036 - val_loss: 0.8662 - val_acc: 0.7473\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8709 - acc: 0.7037 - val_loss: 0.7889 - val_acc: 0.7521\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8687 - acc: 0.7062 - val_loss: 0.8252 - val_acc: 0.7476\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8712 - acc: 0.7040 - val_loss: 0.7526 - val_acc: 0.7656\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8733 - acc: 0.7047 - val_loss: 0.7746 - val_acc: 0.7528\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8747 - acc: 0.7021 - val_loss: 0.8603 - val_acc: 0.7572\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8685 - acc: 0.7060 - val_loss: 0.7870 - val_acc: 0.7484\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8795 - acc: 0.7014 - val_loss: 0.8107 - val_acc: 0.7591\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8672 - acc: 0.7038 - val_loss: 0.7283 - val_acc: 0.7600\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8741 - acc: 0.7025 - val_loss: 0.7814 - val_acc: 0.7552\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8719 - acc: 0.7054 - val_loss: 0.7930 - val_acc: 0.7477\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8674 - acc: 0.7054 - val_loss: 0.7789 - val_acc: 0.7420\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8712 - acc: 0.7034 - val_loss: 0.7435 - val_acc: 0.7549\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8690 - acc: 0.7054 - val_loss: 0.8070 - val_acc: 0.7565\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8667 - acc: 0.7036 - val_loss: 0.8212 - val_acc: 0.7444\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8736 - acc: 0.7055 - val_loss: 0.7762 - val_acc: 0.7665\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8720 - acc: 0.7046 - val_loss: 0.7687 - val_acc: 0.7479\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8702 - acc: 0.7051 - val_loss: 0.7335 - val_acc: 0.7563\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8662 - acc: 0.7067 - val_loss: 0.8841 - val_acc: 0.7558\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8676 - acc: 0.7048 - val_loss: 0.7409 - val_acc: 0.7683\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8744 - acc: 0.7035 - val_loss: 0.7568 - val_acc: 0.7658\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8685 - acc: 0.7046 - val_loss: 0.7577 - val_acc: 0.7576\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8738 - acc: 0.7051 - val_loss: 0.7886 - val_acc: 0.7600\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8681 - acc: 0.7068 - val_loss: 0.7320 - val_acc: 0.7635\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8696 - acc: 0.7059 - val_loss: 0.8147 - val_acc: 0.7576\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8743 - acc: 0.7018 - val_loss: 0.9751 - val_acc: 0.7279\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8750 - acc: 0.7043 - val_loss: 0.7512 - val_acc: 0.7575\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8680 - acc: 0.7060 - val_loss: 0.7766 - val_acc: 0.7604\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8673 - acc: 0.7059 - val_loss: 0.7380 - val_acc: 0.7556\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8802 - acc: 0.7030 - val_loss: 0.7868 - val_acc: 0.7444\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8698 - acc: 0.7057 - val_loss: 0.8212 - val_acc: 0.7552\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8709 - acc: 0.7033 - val_loss: 0.7932 - val_acc: 0.7447\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8738 - acc: 0.7056 - val_loss: 0.7898 - val_acc: 0.7593\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8776 - acc: 0.7035 - val_loss: 0.7655 - val_acc: 0.7611\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8755 - acc: 0.7055 - val_loss: 0.7859 - val_acc: 0.7492\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8734 - acc: 0.7057 - val_loss: 0.7395 - val_acc: 0.7549\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.8667 - acc: 0.7073 - val_loss: 0.7274 - val_acc: 0.7593\n",
      "Saved trained model at D:\\Spring 2018\\AI\\Assignment-1\\saved_models\\keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 1s 135us/step\n",
      "Test loss: 0.7273836276054383\n",
      "Test accuracy: 0.7593\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (2, 2), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy decreased to 75%. Model fits well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Changing padding to valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is increasing to 77% therefore no padding is requires for these images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.9190 - acc: 0.2791 - val_loss: 1.6526 - val_acc: 0.4003\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.6317 - acc: 0.3950 - val_loss: 1.4552 - val_acc: 0.4743\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 1.5014 - acc: 0.4513 - val_loss: 1.3736 - val_acc: 0.4999\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4079 - acc: 0.4905 - val_loss: 1.2432 - val_acc: 0.5576\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.3407 - acc: 0.5193 - val_loss: 1.1839 - val_acc: 0.5771\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2854 - acc: 0.5406 - val_loss: 1.1396 - val_acc: 0.5961\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2345 - acc: 0.5621 - val_loss: 1.0741 - val_acc: 0.6221\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.1921 - acc: 0.5788 - val_loss: 1.0627 - val_acc: 0.6323\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 1.1517 - acc: 0.5924 - val_loss: 1.0277 - val_acc: 0.6449\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.1222 - acc: 0.6035 - val_loss: 0.9837 - val_acc: 0.6549\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.0901 - acc: 0.6148 - val_loss: 0.9519 - val_acc: 0.6685\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0662 - acc: 0.6239 - val_loss: 0.9134 - val_acc: 0.6803\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0387 - acc: 0.6329 - val_loss: 0.9210 - val_acc: 0.6840\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 1.0152 - acc: 0.6424 - val_loss: 0.8679 - val_acc: 0.6935\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.9962 - acc: 0.6494 - val_loss: 0.8810 - val_acc: 0.6891\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.9799 - acc: 0.6552 - val_loss: 0.8326 - val_acc: 0.7075\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.9623 - acc: 0.6600 - val_loss: 0.8486 - val_acc: 0.7013\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.9515 - acc: 0.6660 - val_loss: 0.8036 - val_acc: 0.7187\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.9306 - acc: 0.6738 - val_loss: 0.7963 - val_acc: 0.7242\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.9234 - acc: 0.6780 - val_loss: 0.7916 - val_acc: 0.7266\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.9128 - acc: 0.6811 - val_loss: 0.8053 - val_acc: 0.7149\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.9025 - acc: 0.6859 - val_loss: 0.7652 - val_acc: 0.7352\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8909 - acc: 0.6890 - val_loss: 0.7583 - val_acc: 0.7342\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8829 - acc: 0.6928 - val_loss: 0.7544 - val_acc: 0.7348\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8718 - acc: 0.6959 - val_loss: 0.7484 - val_acc: 0.7376\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.8677 - acc: 0.6974 - val_loss: 0.7380 - val_acc: 0.7425\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8678 - acc: 0.6969 - val_loss: 0.7263 - val_acc: 0.7485\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8582 - acc: 0.7028 - val_loss: 0.7312 - val_acc: 0.7455\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8495 - acc: 0.7054 - val_loss: 0.7400 - val_acc: 0.7385\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.8466 - acc: 0.7054 - val_loss: 0.7067 - val_acc: 0.7579\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8422 - acc: 0.7067 - val_loss: 0.7006 - val_acc: 0.7573\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8379 - acc: 0.7112 - val_loss: 0.7329 - val_acc: 0.7433\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8310 - acc: 0.7122 - val_loss: 0.7093 - val_acc: 0.7592\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8310 - acc: 0.7128 - val_loss: 0.6976 - val_acc: 0.7592\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8256 - acc: 0.7148 - val_loss: 0.7156 - val_acc: 0.7562\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8199 - acc: 0.7163 - val_loss: 0.6894 - val_acc: 0.7613\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8155 - acc: 0.7187 - val_loss: 0.7027 - val_acc: 0.7596\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.8230 - acc: 0.7166 - val_loss: 0.6872 - val_acc: 0.7682\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8181 - acc: 0.7193 - val_loss: 0.6752 - val_acc: 0.7701\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8154 - acc: 0.7206 - val_loss: 0.6777 - val_acc: 0.7689\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8142 - acc: 0.7216 - val_loss: 0.6757 - val_acc: 0.7677\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8094 - acc: 0.7225 - val_loss: 0.6988 - val_acc: 0.7662\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8020 - acc: 0.7251 - val_loss: 0.6606 - val_acc: 0.7718\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8022 - acc: 0.7247 - val_loss: 0.6642 - val_acc: 0.7687\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7989 - acc: 0.7271 - val_loss: 0.6848 - val_acc: 0.7624\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.7937 - acc: 0.7294 - val_loss: 0.6607 - val_acc: 0.7787\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7953 - acc: 0.7270 - val_loss: 0.6551 - val_acc: 0.7749\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7892 - acc: 0.7281 - val_loss: 0.6519 - val_acc: 0.7789\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7897 - acc: 0.7302 - val_loss: 0.6913 - val_acc: 0.7672\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7915 - acc: 0.7280 - val_loss: 0.6729 - val_acc: 0.7634\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7913 - acc: 0.7281 - val_loss: 0.6494 - val_acc: 0.7752\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7867 - acc: 0.7309 - val_loss: 0.6474 - val_acc: 0.7820\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7851 - acc: 0.7341 - val_loss: 0.6528 - val_acc: 0.7748\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7795 - acc: 0.7336 - val_loss: 0.6659 - val_acc: 0.7756\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7842 - acc: 0.7327 - val_loss: 0.7294 - val_acc: 0.7601\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7808 - acc: 0.7347 - val_loss: 0.6466 - val_acc: 0.7789\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7813 - acc: 0.7356 - val_loss: 0.6660 - val_acc: 0.7733\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.7789 - acc: 0.7339 - val_loss: 0.6822 - val_acc: 0.7817\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7794 - acc: 0.7349 - val_loss: 0.6878 - val_acc: 0.7668\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7707 - acc: 0.7387 - val_loss: 0.6369 - val_acc: 0.7860\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7738 - acc: 0.7356 - val_loss: 0.6831 - val_acc: 0.7658\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7729 - acc: 0.7384 - val_loss: 0.6463 - val_acc: 0.7903\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7783 - acc: 0.7369 - val_loss: 0.6219 - val_acc: 0.7877\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7742 - acc: 0.7380 - val_loss: 0.6447 - val_acc: 0.7854\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7704 - acc: 0.7383 - val_loss: 0.6671 - val_acc: 0.7845\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7698 - acc: 0.7385 - val_loss: 0.6463 - val_acc: 0.7861\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.7777 - acc: 0.7366 - val_loss: 0.6539 - val_acc: 0.7810\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7763 - acc: 0.7378 - val_loss: 0.6535 - val_acc: 0.7840\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7798 - acc: 0.7361 - val_loss: 0.6675 - val_acc: 0.7772\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.7721 - acc: 0.7376 - val_loss: 0.6582 - val_acc: 0.7857\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7744 - acc: 0.7365 - val_loss: 0.6537 - val_acc: 0.7821\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7760 - acc: 0.7406 - val_loss: 0.6661 - val_acc: 0.7769\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7721 - acc: 0.7402 - val_loss: 0.6538 - val_acc: 0.7862\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7685 - acc: 0.7405 - val_loss: 0.6548 - val_acc: 0.7850\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7676 - acc: 0.7395 - val_loss: 0.6327 - val_acc: 0.7930\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7714 - acc: 0.7428 - val_loss: 0.7028 - val_acc: 0.7756\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7774 - acc: 0.7408 - val_loss: 0.7076 - val_acc: 0.7772\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.7765 - acc: 0.7395 - val_loss: 0.6305 - val_acc: 0.7872\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7659 - acc: 0.7422 - val_loss: 0.6724 - val_acc: 0.7761\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7709 - acc: 0.7411 - val_loss: 0.7159 - val_acc: 0.7733\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7742 - acc: 0.7409 - val_loss: 0.6682 - val_acc: 0.7852\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7702 - acc: 0.7409 - val_loss: 0.6880 - val_acc: 0.7646\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7707 - acc: 0.7407 - val_loss: 0.6668 - val_acc: 0.7800\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7738 - acc: 0.7404 - val_loss: 0.7307 - val_acc: 0.7618\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7714 - acc: 0.7414 - val_loss: 0.7165 - val_acc: 0.7657\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.7793 - acc: 0.7373 - val_loss: 0.7456 - val_acc: 0.7820\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7733 - acc: 0.7430 - val_loss: 0.7375 - val_acc: 0.7637\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7716 - acc: 0.7418 - val_loss: 0.6630 - val_acc: 0.7775\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7792 - acc: 0.7404 - val_loss: 0.6795 - val_acc: 0.7825\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7762 - acc: 0.7398 - val_loss: 0.6679 - val_acc: 0.7787\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7767 - acc: 0.7414 - val_loss: 0.6274 - val_acc: 0.7905\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7741 - acc: 0.7412 - val_loss: 0.6843 - val_acc: 0.7813\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7920 - acc: 0.7352 - val_loss: 0.6828 - val_acc: 0.7860\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7757 - acc: 0.7415 - val_loss: 0.6739 - val_acc: 0.7794\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7844 - acc: 0.7364 - val_loss: 0.7752 - val_acc: 0.7719\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7833 - acc: 0.7380 - val_loss: 0.6469 - val_acc: 0.7857\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7854 - acc: 0.7389 - val_loss: 0.6449 - val_acc: 0.7870\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7872 - acc: 0.7398 - val_loss: 0.6984 - val_acc: 0.7829\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7897 - acc: 0.7390 - val_loss: 0.7096 - val_acc: 0.7696\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7953 - acc: 0.7364 - val_loss: 0.6895 - val_acc: 0.7718\n",
      "Saved trained model at D:\\Spring 2018\\AI\\Assignment-1\\saved_models\\keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 1s 124us/step\n",
      "Test loss: 0.689486971950531\n",
      "Test accuracy: 0.7718\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='valid',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Activation to sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy decreases to 56%. Relu gives better results than sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 2.3300 - acc: 0.0974 - val_loss: 2.3071 - val_acc: 0.1000\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 2.3074 - acc: 0.1007 - val_loss: 2.3039 - val_acc: 0.1000\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 2.3065 - acc: 0.0992 - val_loss: 2.3033 - val_acc: 0.1000\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 2.3056 - acc: 0.0996 - val_loss: 2.3031 - val_acc: 0.1000\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 2.3053 - acc: 0.0998 - val_loss: 2.3030 - val_acc: 0.1000\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 2.3049 - acc: 0.1001 - val_loss: 2.3027 - val_acc: 0.1000\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 2.3044 - acc: 0.1010 - val_loss: 2.3032 - val_acc: 0.1000\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 2.3030 - acc: 0.1033 - val_loss: 2.2757 - val_acc: 0.1437\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 2.1533 - acc: 0.1944 - val_loss: 2.0200 - val_acc: 0.2586\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 2.0524 - acc: 0.2392 - val_loss: 1.9708 - val_acc: 0.2698\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 2.0117 - acc: 0.2570 - val_loss: 1.9341 - val_acc: 0.2847\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.9819 - acc: 0.2673 - val_loss: 1.9032 - val_acc: 0.3058\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.9608 - acc: 0.2784 - val_loss: 1.8760 - val_acc: 0.3069\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.9416 - acc: 0.2877 - val_loss: 1.8557 - val_acc: 0.3197\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.9269 - acc: 0.2914 - val_loss: 1.8330 - val_acc: 0.3368\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.9146 - acc: 0.2973 - val_loss: 1.8187 - val_acc: 0.3408\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.8989 - acc: 0.3057 - val_loss: 1.8047 - val_acc: 0.3474\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.8928 - acc: 0.3092 - val_loss: 1.8001 - val_acc: 0.3474\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.8869 - acc: 0.3103 - val_loss: 1.7848 - val_acc: 0.3533\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.8816 - acc: 0.3151 - val_loss: 1.7758 - val_acc: 0.3567\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.8726 - acc: 0.3183 - val_loss: 1.7685 - val_acc: 0.3623\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.8668 - acc: 0.3188 - val_loss: 1.7654 - val_acc: 0.3631\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.8613 - acc: 0.3218 - val_loss: 1.7700 - val_acc: 0.3625\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.8571 - acc: 0.3236 - val_loss: 1.7533 - val_acc: 0.3741\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.8524 - acc: 0.3250 - val_loss: 1.7474 - val_acc: 0.3764\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.8439 - acc: 0.3260 - val_loss: 1.7422 - val_acc: 0.3748\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.8433 - acc: 0.3287 - val_loss: 1.7304 - val_acc: 0.3712\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.8325 - acc: 0.3322 - val_loss: 1.7252 - val_acc: 0.3827\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.8265 - acc: 0.3338 - val_loss: 1.7111 - val_acc: 0.3852\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.8180 - acc: 0.3361 - val_loss: 1.7043 - val_acc: 0.3891\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.8119 - acc: 0.3399 - val_loss: 1.6951 - val_acc: 0.3868\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.8025 - acc: 0.3436 - val_loss: 1.6817 - val_acc: 0.3902\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.7879 - acc: 0.3479 - val_loss: 1.6589 - val_acc: 0.4040\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.7731 - acc: 0.3528 - val_loss: 1.6585 - val_acc: 0.4077\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.7562 - acc: 0.3606 - val_loss: 1.6142 - val_acc: 0.4188\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.7409 - acc: 0.3659 - val_loss: 1.6082 - val_acc: 0.4242\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.7263 - acc: 0.3715 - val_loss: 1.6117 - val_acc: 0.4192\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.7162 - acc: 0.3750 - val_loss: 1.6299 - val_acc: 0.4260\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.7058 - acc: 0.3810 - val_loss: 1.5687 - val_acc: 0.4457\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.6937 - acc: 0.3857 - val_loss: 1.5419 - val_acc: 0.4519\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.6771 - acc: 0.3934 - val_loss: 1.5389 - val_acc: 0.4541\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.6707 - acc: 0.3915 - val_loss: 1.5250 - val_acc: 0.4566\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.6573 - acc: 0.4011 - val_loss: 1.5061 - val_acc: 0.4621\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.6472 - acc: 0.4060 - val_loss: 1.5166 - val_acc: 0.4555\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.6392 - acc: 0.4071 - val_loss: 1.4881 - val_acc: 0.4658\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.6294 - acc: 0.4097 - val_loss: 1.4693 - val_acc: 0.4756\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.6203 - acc: 0.4128 - val_loss: 1.4607 - val_acc: 0.4753\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.6068 - acc: 0.4198 - val_loss: 1.4535 - val_acc: 0.4788\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5977 - acc: 0.4249 - val_loss: 1.4571 - val_acc: 0.4749\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5890 - acc: 0.4232 - val_loss: 1.4357 - val_acc: 0.4825\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.5889 - acc: 0.4276 - val_loss: 1.4481 - val_acc: 0.4836\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.5752 - acc: 0.4336 - val_loss: 1.4232 - val_acc: 0.4874\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.5714 - acc: 0.4338 - val_loss: 1.4223 - val_acc: 0.4914\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5659 - acc: 0.4345 - val_loss: 1.4149 - val_acc: 0.4913\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5596 - acc: 0.4366 - val_loss: 1.4028 - val_acc: 0.5029\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5524 - acc: 0.4424 - val_loss: 1.4062 - val_acc: 0.4983\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5449 - acc: 0.4442 - val_loss: 1.3965 - val_acc: 0.4993\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.5505 - acc: 0.4434 - val_loss: 1.4074 - val_acc: 0.4960\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5404 - acc: 0.4488 - val_loss: 1.3799 - val_acc: 0.5039\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5386 - acc: 0.4501 - val_loss: 1.3803 - val_acc: 0.5084\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5339 - acc: 0.4504 - val_loss: 1.3939 - val_acc: 0.5013\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5285 - acc: 0.4520 - val_loss: 1.3859 - val_acc: 0.5059\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5206 - acc: 0.4525 - val_loss: 1.3689 - val_acc: 0.5104\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5200 - acc: 0.4552 - val_loss: 1.3604 - val_acc: 0.5195\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5146 - acc: 0.4567 - val_loss: 1.3537 - val_acc: 0.5179\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.5132 - acc: 0.4561 - val_loss: 1.3496 - val_acc: 0.5228\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.5098 - acc: 0.4578 - val_loss: 1.3578 - val_acc: 0.5171\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.5073 - acc: 0.4575 - val_loss: 1.3502 - val_acc: 0.5185\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4992 - acc: 0.4620 - val_loss: 1.3438 - val_acc: 0.5227\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4948 - acc: 0.4655 - val_loss: 1.3335 - val_acc: 0.5245\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4922 - acc: 0.4630 - val_loss: 1.3342 - val_acc: 0.5298\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4856 - acc: 0.4677 - val_loss: 1.3319 - val_acc: 0.5244\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4893 - acc: 0.4669 - val_loss: 1.3171 - val_acc: 0.5318\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4800 - acc: 0.4713 - val_loss: 1.3155 - val_acc: 0.5327\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4744 - acc: 0.4734 - val_loss: 1.3023 - val_acc: 0.5365\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4787 - acc: 0.4697 - val_loss: 1.3113 - val_acc: 0.5406\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4684 - acc: 0.4723 - val_loss: 1.3226 - val_acc: 0.5321\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4668 - acc: 0.4734 - val_loss: 1.2943 - val_acc: 0.5381\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4711 - acc: 0.4726 - val_loss: 1.3194 - val_acc: 0.5307\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4633 - acc: 0.4751 - val_loss: 1.3093 - val_acc: 0.5313\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4675 - acc: 0.4761 - val_loss: 1.2911 - val_acc: 0.5451\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4596 - acc: 0.4781 - val_loss: 1.2882 - val_acc: 0.5430\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4614 - acc: 0.4782 - val_loss: 1.2964 - val_acc: 0.5430\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4578 - acc: 0.4766 - val_loss: 1.2912 - val_acc: 0.5424\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4530 - acc: 0.4827 - val_loss: 1.2762 - val_acc: 0.5495\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4538 - acc: 0.4806 - val_loss: 1.2756 - val_acc: 0.5449\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4406 - acc: 0.4853 - val_loss: 1.2728 - val_acc: 0.5492\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4415 - acc: 0.4846 - val_loss: 1.2819 - val_acc: 0.5492\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4382 - acc: 0.4850 - val_loss: 1.2730 - val_acc: 0.5473\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4327 - acc: 0.4872 - val_loss: 1.2697 - val_acc: 0.5488\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4352 - acc: 0.4864 - val_loss: 1.2765 - val_acc: 0.5514\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4293 - acc: 0.4872 - val_loss: 1.2633 - val_acc: 0.5525\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4226 - acc: 0.4899 - val_loss: 1.2513 - val_acc: 0.5527\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4255 - acc: 0.4904 - val_loss: 1.2471 - val_acc: 0.5567\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4228 - acc: 0.4909 - val_loss: 1.2480 - val_acc: 0.5537\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4198 - acc: 0.4927 - val_loss: 1.2426 - val_acc: 0.5556\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4167 - acc: 0.4943 - val_loss: 1.2419 - val_acc: 0.5626\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4184 - acc: 0.4922 - val_loss: 1.2564 - val_acc: 0.5569\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4128 - acc: 0.4966 - val_loss: 1.2353 - val_acc: 0.5553\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4119 - acc: 0.4947 - val_loss: 1.2374 - val_acc: 0.5628\n",
      "Saved trained model at D:\\Spring 2018\\AI\\Assignment-1\\saved_models\\keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 1s 147us/step\n",
      "Test loss: 1.2373537200927733\n",
      "Test accuracy: 0.5628\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\tChanging stride value to [2,2] from default [1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is at 73% which is not a bad model. Moving one step at a time gives a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 2.1201 - acc: 0.1951 - val_loss: 1.9309 - val_acc: 0.2874\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.8740 - acc: 0.2949 - val_loss: 1.7349 - val_acc: 0.3741\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.7376 - acc: 0.3515 - val_loss: 1.5956 - val_acc: 0.4151\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.6503 - acc: 0.3875 - val_loss: 1.4962 - val_acc: 0.4543\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5917 - acc: 0.4109 - val_loss: 1.4370 - val_acc: 0.4710\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.5428 - acc: 0.4335 - val_loss: 1.3959 - val_acc: 0.4967\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.5088 - acc: 0.4455 - val_loss: 1.3895 - val_acc: 0.4950\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.4701 - acc: 0.4622 - val_loss: 1.3100 - val_acc: 0.5280\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4406 - acc: 0.4722 - val_loss: 1.2844 - val_acc: 0.5371\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4123 - acc: 0.4838 - val_loss: 1.2588 - val_acc: 0.5473\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3904 - acc: 0.4955 - val_loss: 1.2467 - val_acc: 0.5491\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.3627 - acc: 0.5040 - val_loss: 1.2360 - val_acc: 0.5538\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3442 - acc: 0.5142 - val_loss: 1.1845 - val_acc: 0.5730\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3204 - acc: 0.5223 - val_loss: 1.1659 - val_acc: 0.5802\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3035 - acc: 0.5286 - val_loss: 1.1397 - val_acc: 0.5897\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.2819 - acc: 0.5352 - val_loss: 1.1299 - val_acc: 0.5875\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2703 - acc: 0.5406 - val_loss: 1.1257 - val_acc: 0.6010\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 1.2484 - acc: 0.5507 - val_loss: 1.0986 - val_acc: 0.6106\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 1.2326 - acc: 0.5547 - val_loss: 1.0805 - val_acc: 0.6083\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2210 - acc: 0.5601 - val_loss: 1.0823 - val_acc: 0.6121\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2046 - acc: 0.5681 - val_loss: 1.0403 - val_acc: 0.6275\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1957 - acc: 0.5737 - val_loss: 1.0396 - val_acc: 0.6271\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.1824 - acc: 0.5765 - val_loss: 1.0309 - val_acc: 0.6412\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 1.1697 - acc: 0.5850 - val_loss: 1.0574 - val_acc: 0.6295\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.1568 - acc: 0.5866 - val_loss: 1.0039 - val_acc: 0.6471\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.1473 - acc: 0.5919 - val_loss: 0.9949 - val_acc: 0.6445\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1395 - acc: 0.5966 - val_loss: 0.9920 - val_acc: 0.6541\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1293 - acc: 0.5996 - val_loss: 0.9805 - val_acc: 0.6562\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.1165 - acc: 0.6019 - val_loss: 0.9607 - val_acc: 0.6625\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.1065 - acc: 0.6071 - val_loss: 0.9557 - val_acc: 0.6652\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.1053 - acc: 0.6085 - val_loss: 0.9629 - val_acc: 0.6611\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.0969 - acc: 0.6122 - val_loss: 0.9294 - val_acc: 0.6740\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.0920 - acc: 0.6141 - val_loss: 0.9344 - val_acc: 0.6702\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0838 - acc: 0.6160 - val_loss: 0.9174 - val_acc: 0.6796\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.0720 - acc: 0.6192 - val_loss: 0.9112 - val_acc: 0.6786\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.0655 - acc: 0.6239 - val_loss: 0.9181 - val_acc: 0.6765\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.0640 - acc: 0.6222 - val_loss: 0.9200 - val_acc: 0.6780\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.0557 - acc: 0.6275 - val_loss: 0.8988 - val_acc: 0.6878\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.0540 - acc: 0.6300 - val_loss: 0.9021 - val_acc: 0.6863\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.0484 - acc: 0.6287 - val_loss: 0.9113 - val_acc: 0.6812\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.0421 - acc: 0.6325 - val_loss: 0.8951 - val_acc: 0.6844\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.0368 - acc: 0.6353 - val_loss: 0.8748 - val_acc: 0.6968\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.0304 - acc: 0.6386 - val_loss: 0.8822 - val_acc: 0.6882\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.0248 - acc: 0.6402 - val_loss: 0.8815 - val_acc: 0.6919\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.0243 - acc: 0.6406 - val_loss: 0.8826 - val_acc: 0.6932\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0155 - acc: 0.6428 - val_loss: 0.8536 - val_acc: 0.7044\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0076 - acc: 0.6470 - val_loss: 0.8486 - val_acc: 0.7047\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0101 - acc: 0.6462 - val_loss: 0.8818 - val_acc: 0.6986\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0037 - acc: 0.6479 - val_loss: 0.8484 - val_acc: 0.7083\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.9986 - acc: 0.6491 - val_loss: 0.8458 - val_acc: 0.7066\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.9994 - acc: 0.6533 - val_loss: 0.8289 - val_acc: 0.7136\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9952 - acc: 0.6535 - val_loss: 0.8567 - val_acc: 0.7026\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9885 - acc: 0.6556 - val_loss: 0.8607 - val_acc: 0.7078\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 0.9899 - acc: 0.6557 - val_loss: 0.8264 - val_acc: 0.7116\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.9834 - acc: 0.6574 - val_loss: 0.8339 - val_acc: 0.7134\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9823 - acc: 0.6588 - val_loss: 0.8258 - val_acc: 0.7157\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9794 - acc: 0.6587 - val_loss: 0.8260 - val_acc: 0.7142\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 0.9768 - acc: 0.6607 - val_loss: 0.8390 - val_acc: 0.7111\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9743 - acc: 0.6612 - val_loss: 0.8405 - val_acc: 0.7101\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9715 - acc: 0.6657 - val_loss: 0.8330 - val_acc: 0.7160\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9713 - acc: 0.6638 - val_loss: 0.8372 - val_acc: 0.7122\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9643 - acc: 0.6664 - val_loss: 0.8281 - val_acc: 0.7192\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9630 - acc: 0.6672 - val_loss: 0.8239 - val_acc: 0.7185\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9664 - acc: 0.6642 - val_loss: 0.8143 - val_acc: 0.7194\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9581 - acc: 0.6677 - val_loss: 0.7911 - val_acc: 0.7277\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9515 - acc: 0.6728 - val_loss: 0.7941 - val_acc: 0.7261\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9512 - acc: 0.6725 - val_loss: 0.8090 - val_acc: 0.7271\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.9531 - acc: 0.6703 - val_loss: 0.8014 - val_acc: 0.7278\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9523 - acc: 0.6718 - val_loss: 0.8296 - val_acc: 0.7182\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9540 - acc: 0.6731 - val_loss: 0.8487 - val_acc: 0.7102\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9439 - acc: 0.6759 - val_loss: 0.8085 - val_acc: 0.7208\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9494 - acc: 0.6715 - val_loss: 0.8073 - val_acc: 0.7277\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9443 - acc: 0.6739 - val_loss: 0.7875 - val_acc: 0.7275\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9394 - acc: 0.6760 - val_loss: 0.7932 - val_acc: 0.7316\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9455 - acc: 0.6732 - val_loss: 0.8239 - val_acc: 0.7145\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9374 - acc: 0.6766 - val_loss: 0.7788 - val_acc: 0.7322\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9411 - acc: 0.6785 - val_loss: 0.8317 - val_acc: 0.7190\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9390 - acc: 0.6777 - val_loss: 0.7812 - val_acc: 0.7301\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9377 - acc: 0.6776 - val_loss: 0.7967 - val_acc: 0.7311\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9424 - acc: 0.6746 - val_loss: 0.7955 - val_acc: 0.7272\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9356 - acc: 0.6796 - val_loss: 0.7989 - val_acc: 0.7253\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9320 - acc: 0.6813 - val_loss: 0.8047 - val_acc: 0.7328\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9315 - acc: 0.6835 - val_loss: 0.7983 - val_acc: 0.7268\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9348 - acc: 0.6813 - val_loss: 0.8029 - val_acc: 0.7314\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9353 - acc: 0.6809 - val_loss: 0.7880 - val_acc: 0.7333\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9312 - acc: 0.6793 - val_loss: 0.7630 - val_acc: 0.7422\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9300 - acc: 0.6794 - val_loss: 0.8102 - val_acc: 0.7266\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9303 - acc: 0.6816 - val_loss: 0.7798 - val_acc: 0.7357\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9298 - acc: 0.6822 - val_loss: 0.8082 - val_acc: 0.7252\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9303 - acc: 0.6817 - val_loss: 0.8093 - val_acc: 0.7272\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9316 - acc: 0.6806 - val_loss: 0.7725 - val_acc: 0.7400\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9244 - acc: 0.6836 - val_loss: 0.7909 - val_acc: 0.7321\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9265 - acc: 0.6834 - val_loss: 0.8091 - val_acc: 0.7294\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9269 - acc: 0.6833 - val_loss: 0.7772 - val_acc: 0.7406\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9188 - acc: 0.6855 - val_loss: 0.7838 - val_acc: 0.7420\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9226 - acc: 0.6850 - val_loss: 0.8027 - val_acc: 0.7308\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9245 - acc: 0.6851 - val_loss: 0.8071 - val_acc: 0.7343\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9264 - acc: 0.6857 - val_loss: 0.8124 - val_acc: 0.7268\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9226 - acc: 0.6862 - val_loss: 0.7932 - val_acc: 0.7326\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.9228 - acc: 0.6856 - val_loss: 0.7843 - val_acc: 0.7397\n",
      "Saved trained model at D:\\Spring 2018\\AI\\Assignment-1\\saved_models\\keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 1s 113us/step\n",
      "Test loss: 0.7842530988693237\n",
      "Test accuracy: 0.7397\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', strides=(2, 2), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
